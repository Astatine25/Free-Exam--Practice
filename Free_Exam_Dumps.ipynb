{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Scraper:\n",
        "    def __init__(self, provider):\n",
        "        self.session = requests.Session()\n",
        "        # Pretend to be a real Chrome browser\n",
        "        self.session.headers.update({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "        })\n",
        "        self.provider = provider.lower()\n",
        "        self.base_url = f\"https://www.examtopics.com/discussions/{self.provider}/\"\n",
        "\n",
        "    def get_num_pages(self):\n",
        "        try:\n",
        "            response = self.session.get(self.base_url, timeout=15)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Connection failed: Status {response.status_code}. Site may be blocking scripts.\")\n",
        "                return 0\n",
        "\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            indicator = soup.find(\"span\", {\"class\": \"discussion-list-page-indicator\"})\n",
        "\n",
        "            if indicator:\n",
        "                # Extracts the '571' from 'Page 1 of 571'\n",
        "                text = indicator.get_text(strip=True)\n",
        "                total_pages = int(text.split(\"of\")[-1].strip())\n",
        "                return total_pages\n",
        "            return 0\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page count: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def fetch_page_links(self, page, search_string):\n",
        "        try:\n",
        "            # Adding a tiny delay to avoid triggering anti-bot\n",
        "            time.sleep(0.5)\n",
        "            response = self.session.get(f\"{self.base_url}{page}/\", timeout=10)\n",
        "            if response.status_code != 200:\n",
        "                return []\n",
        "\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            discussions = soup.find_all(\"a\", {\"class\": \"discussion-link\"})\n",
        "\n",
        "            links = []\n",
        "            for d in discussions:\n",
        "                if search_string in d.text.upper():\n",
        "                    full_link = \"https://www.examtopics.com\" + d[\"href\"]\n",
        "                    links.append(full_link)\n",
        "            return links\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    def get_discussion_links(self, num_pages, search_string):\n",
        "        links = []\n",
        "        # max_workers=5 is safer to avoid getting your IP banned\n",
        "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            futures = [executor.submit(self.fetch_page_links, page, search_string) for page in range(1, num_pages + 1)]\n",
        "            with tqdm(total=num_pages, desc=\"Searching Pages\", unit=\"page\") as pbar:\n",
        "                for future in as_completed(futures):\n",
        "                    page_links = future.result()\n",
        "                    links.extend(page_links)\n",
        "                    pbar.update(1)\n",
        "        return links\n",
        "\n",
        "def extract_topic_question(link):\n",
        "    # Regex to find Topic X and Question Y in the URL\n",
        "    match = re.search(r'topic-(\\d+)-question-(\\d+)', link)\n",
        "    return (int(match.group(1)), int(match.group(2))) if match else (999, 999)\n",
        "\n",
        "def write_grouped_links_to_file(filename, links):\n",
        "    # Sort by Topic, then by Question\n",
        "    sorted_links = sorted(list(set(links)), key=extract_topic_question)\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        current_topic = None\n",
        "        for link in sorted_links:\n",
        "            topic, q = extract_topic_question(link)\n",
        "            if topic != current_topic:\n",
        "                f.write(f\"\\n--- TOPIC {topic} ---\\n\")\n",
        "                current_topic = topic\n",
        "            f.write(f\"{link}\\n\")\n",
        "\n",
        "def main():\n",
        "    print(\"--- ExamTopics Scraper ---\")\n",
        "    provider = input(\"Enter provider (e.g., amazon, microsoft): \").strip()\n",
        "    scraper = Scraper(provider)\n",
        "\n",
        "    num_pages = scraper.get_num_pages()\n",
        "    if num_pages == 0:\n",
        "        # Fallback: sometimes the scraper can't see the page count but can see the pages\n",
        "        num_pages = int(input(\"Could not detect page count. How many pages should I search? (e.g. 100): \"))\n",
        "\n",
        "    print(f\"Searching through {num_pages} pages...\")\n",
        "    search_string = input(\"Enter exam code (e.g., AIF-C01): \").strip().upper()\n",
        "\n",
        "    links = scraper.get_discussion_links(num_pages, search_string)\n",
        "\n",
        "    if links:\n",
        "        filename = f\"{search_string}_links.txt\"\n",
        "        write_grouped_links_to_file(filename, links)\n",
        "        print(f\"\\nSuccess! Found {len(links)} links. Saved to {filename}\")\n",
        "    else:\n",
        "        print(\"\\nNo links found. Check if the exam code is correct or if the site is blocking requests.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcOQeZXwC0zA",
        "outputId": "a817b90b-3701-4cfc-fbbf-1134689c1c48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ExamTopics Scraper ---\n",
            "Enter provider (e.g., amazon, microsoft): amazon\n",
            "Searching through 571 pages...\n",
            "Enter exam code (e.g., AIF-C01): AIF-C01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Searching Pages: 100%|██████████| 571/571 [04:23<00:00,  2.17page/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success! Found 323 links. Saved to AIF-C01_links.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1wLZnQzOJvka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rgQUUHvCwh3",
        "outputId": "27700315-6fd7-4422-ff2c-b9b51bd17115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to scrape 323 questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping Content: 100%|██████████| 323/323 [13:18<00:00,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished! Your data is saved in AIF-C01_Questions_Answers.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "INPUT_FILE = 'AIF-C01_links.txt'\n",
        "OUTPUT_FILE = 'AIF-C01_Questions_Answers.txt'\n",
        "\n",
        "# Use the same headers to avoid bot detection\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def scrape_question_details(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # 1. Extract Question Text\n",
        "        question_text = soup.find('p', class_='card-text').get_text(strip=True)\n",
        "\n",
        "        # 2. Extract Options\n",
        "        options = []\n",
        "        option_list = soup.find_all('li', class_='multi-choice-item')\n",
        "        for opt in option_list:\n",
        "            options.append(opt.get_text(strip=True))\n",
        "\n",
        "        # 3. Extract \"Most Voted\" Answer from Discussion\n",
        "        # Note: The 'official' answer is often hidden, but the community 'most voted' is usually better\n",
        "        voted_answer = \"Answer not found\"\n",
        "        answer_element = soup.find('span', class_='most-voted-answer-badge')\n",
        "        if answer_element:\n",
        "            # Usually looks like \"Most Voted: A\"\n",
        "            voted_answer = answer_element['title'] if 'title' in answer_element.attrs else answer_element.get_text(strip=True)\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'question': question_text,\n",
        "            'options': options,\n",
        "            'answer': voted_answer\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    # Read links from your file\n",
        "    with open(INPUT_FILE, 'r') as f:\n",
        "        links = [line.strip() for line in f if line.startswith('http')]\n",
        "\n",
        "    print(f\"Starting to scrape {len(links)} questions...\")\n",
        "\n",
        "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:\n",
        "        for link in tqdm(links, desc=\"Scraping Content\"):\n",
        "            data = scrape_question_details(link)\n",
        "            if data:\n",
        "                out.write(f\"URL: {data['url']}\\n\")\n",
        "                out.write(f\"QUESTION: {data['question']}\\n\")\n",
        "                out.write(\"OPTIONS:\\n\")\n",
        "                for opt in data['options']:\n",
        "                    out.write(f\" - {opt}\\n\")\n",
        "                out.write(f\"COMMUNITY ANSWER: {data['answer']}\\n\")\n",
        "                out.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            # Anti-ban delay\n",
        "            time.sleep(1.5)\n",
        "\n",
        "    print(f\"\\nFinished! Your data is saved in {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- SETTINGS ---\n",
        "# Adjust sleep to avoid IP bans (1.5 - 2.0s is safer for large exams)\n",
        "SLEEP_BETWEEN_REQUESTS = 1.5\n",
        "\n",
        "class ExamScraper:\n",
        "    def __init__(self, provider):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "        })\n",
        "        self.provider = provider.lower()\n",
        "        self.base_url = f\"https://www.examtopics.com/discussions/{self.provider}/\"\n",
        "\n",
        "    def get_num_pages(self):\n",
        "        try:\n",
        "            response = self.session.get(self.base_url, timeout=15)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            indicator = soup.find(\"span\", {\"class\": \"discussion-list-page-indicator\"})\n",
        "            if indicator:\n",
        "                text = indicator.get_text(strip=True)\n",
        "                return int(text.split(\"of\")[-1].strip())\n",
        "            return 0\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page count: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def fetch_links_from_page(self, page, search_string):\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            url = f\"{self.base_url}{page}/\"\n",
        "            response = self.session.get(url, timeout=10)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            discussions = soup.find_all(\"a\", {\"class\": \"discussion-link\"})\n",
        "\n",
        "            links = []\n",
        "            for d in discussions:\n",
        "                if search_string in d.text.upper():\n",
        "                    links.append(\"https://www.examtopics.com\" + d[\"href\"])\n",
        "            return links\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def scrape_question_content(self, url):\n",
        "        try:\n",
        "            time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            if response.status_code != 200: return None\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract Question\n",
        "            q_elem = soup.find('p', class_='card-text')\n",
        "            question = q_elem.get_text(strip=True) if q_elem else \"N/A\"\n",
        "\n",
        "            # Extract Options\n",
        "            options = [\" \".join(opt.get_text().split()) for opt in soup.find_all('li', class_='multi-choice-item')]\n",
        "\n",
        "            # Extract Answers (Community & Official)\n",
        "            community_ans = \"N/A\"\n",
        "            vote_badge = soup.find('span', class_='most-voted-answer-badge')\n",
        "            if vote_badge:\n",
        "                community_ans = vote_badge.get('title', vote_badge.get_text(strip=True))\n",
        "\n",
        "            official_ans = \"N/A\"\n",
        "            off_elem = soup.find('span', class_='correct-answer')\n",
        "            if off_elem:\n",
        "                official_ans = off_elem.get_text(strip=True)\n",
        "\n",
        "            return {\n",
        "                'url': url,\n",
        "                'question': question,\n",
        "                'options': options,\n",
        "                'community': community_ans,\n",
        "                'official': official_ans\n",
        "            }\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def extract_topic_q(link):\n",
        "    match = re.search(r'topic-(\\d+)-question-(\\d+)', link)\n",
        "    return (int(match.group(1)), int(match.group(2))) if match else (999, 999)\n",
        "\n",
        "def main():\n",
        "    provider = input(\"Enter provider (e.g., amazon): \").strip()\n",
        "    exam_code = input(\"Enter exam code (e.g., AIF-C01): \").strip().upper()\n",
        "\n",
        "    scraper = ExamScraper(provider)\n",
        "    total_p = scraper.get_num_pages()\n",
        "\n",
        "    if total_p == 0:\n",
        "        total_p = int(input(\"Total pages not found. Enter manual scan range (e.g. 50): \"))\n",
        "\n",
        "    # 1. FETCH ALL LINKS\n",
        "    print(f\"\\n--- Phase 1: Finding {exam_code} Links ---\")\n",
        "    all_links = []\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = [executor.submit(scraper.fetch_links_from_page, p, exam_code) for p in range(1, total_p + 1)]\n",
        "        for f in tqdm(as_completed(futures), total=total_p):\n",
        "            all_links.extend(f.result())\n",
        "\n",
        "    unique_links = sorted(list(set(all_links)), key=extract_topic_q)\n",
        "    print(f\"Found {len(unique_links)} questions.\")\n",
        "\n",
        "    # 2. FETCH CONTENT FOR EACH LINK\n",
        "    print(f\"\\n--- Phase 2: Scraping Questions & Answers ---\")\n",
        "    output_file = f\"{exam_code}_Study_Guide.txt\"\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for link in tqdm(unique_links):\n",
        "            data = scraper.scrape_question_content(link)\n",
        "            if data:\n",
        "                f.write(f\"URL: {data['url']}\\n\")\n",
        "                f.write(f\"QUESTION: {data['question']}\\n\")\n",
        "                f.write(\"OPTIONS:\\n\")\n",
        "                for opt in data['options']: f.write(f\"  {opt}\\n\")\n",
        "                f.write(f\"COMMUNITY VOTED: {data['community']}\\n\")\n",
        "                f.write(f\"OFFICIAL ANSWER: {data['official']}\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\nSuccess! File saved as: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJXXzAg5E2JN",
        "outputId": "ebc9d4b5-9293-44fd-9990-c7f741b1b1d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Phase 1: Finding AIF-C01 Links ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 571/571 [04:23<00:00,  2.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 323 questions.\n",
            "\n",
            "--- Phase 2: Scraping Questions & Answers ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 323/323 [13:05<00:00,  2.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success! File saved as: AIF-C01_Study_Guide.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_yBPgB3KuU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}